#!/usr/bin/env python3

"""
    Código para descubrir subdominios de una web, los subdirectorios, una araña
    para extraer info del html
"""

import requests
import re
import urlparse
from BeautifulSoup import Beautiful

url = "google.com"


class Scanner:

    def __init__(self, url, ignore_list):
        self.session = requests.Session()
        self.target_url = url
        self.directory_list = []
        self.list_to_ignore = ignore_list

    def request(self, url: str = None, params=None):
        """
            Realiza peticiones get
        """
        if url:
            target = url
        else:
            target = self.target_url
        try:
            if params:
                return self.session.get(target, params=params)
            else:
                return self.session.get(target)
        except requests.exceptions.ConnectionError:
            pass

    def post(self, url, form, value):
        action = form.get("action")
        post_url = urlparse.urljoin(self.target_url, action)
        method = form.get("method")
        input_list = form.findAll("input")
        input_list = {}
        for input in input_list:
            in_name = input.get("name")
            in_type = input.get("type")
            in_value = input.get("value")

            if in_type == "test":
                in_value = value

            input_list[in_name] = in_value

        if method == "post":
            response = self.session.post(post_url, data=input_list)

        return response

    def discover_subdomains(self, dic_path):
        """
            Descubrirá subdominos por un ataque por diccionario
        """
        with open(dic_path, "r") as dic_file:
            for line in dic_file:
                # Eliminamos los espacios y saltos de línea
                word = line.strip()
                response = self.request(word + "." + self.target_url)
                domains = []
                if response:
                    domains.append(response)
        return domains

    def discover_directories(self, dic_path):
        """
            Descubrirá parte de la estructura de carpetas del directorio web
        """
        with open(dic_path, "r") as dic_file:
            for line in dic_file:
                # Eliminamos los espacios y saltos de línea
                folder = line.strip()
                response = self.request(self.target_url + "/" + folder)
                domains = []
                if response:
                    domains.append(response)
        return domains

    def spider(self, url=None):
        """
            Extraerá todas la rutas disponibles en el código html de una web
        """
        if url is None:
            url = self.target_url
        response = self.request(url)
        href_links = re.findall('(?:href=")(.*?)"', response.content)
        for link in href_links:
            # Convertimos las rutas relativa en absolutas
            link = urlparse.urljoin(url, link)

            # Eliminamos las autoreferencias
            if '#' in link:
                link = link.split('#')[0]

            # Eliminamos las rutas externas a la web que estamos analizando
            if url in link
            and link not in self.directory_list
            and url not in self.list_to_ignore:
                self.directory_list.append(link)
                # Buscamos en los subdirectorios
                self.directory_list.append(self.spider(link))

        self.directory_list = self.directory_list.sort()

    def find_forms(self):
        get_page = Beautiful(self.request(self.target_url).content)
        form_list = get_page.findAll("form")
        response = []

        for form in form_list:
            action = form.get("action")
            post_url = urlparse.urljoin(self.target_url, action)
            method = form.get("method")
            input_list = form.findAll("input")
            if method == "post":
                response.append(self.post(post_url, form, "test"))
            else:
                response.append(self.request(input_list))
        return response.content

    def run(self):
        for link in self.target_url:
            forms = self.find_forms()
            for form in forms:
                print("The form " + form
                      + " is vulnerable?\t"
                      + self.xss_form_vulnerable(form, link))

        if "=" in link:
            print("The link " + link
                  + " is vulnerable?\t"
                  + self.xss_path_vulnerable(link))

    def xss_path_vulnerable(self, url: str):
        xss = "<script>alert('test')</script>"
        url = url.replace("=", "=" + xss)
        response = self.request(url=url)
        return xss in response.content

    def xss_form_vulnerable(self, form, url):
        xss = "<script>alert('test')</script>"
        response = self.post(url, form, xss)
        return xss in response.content


dic_path = "ruta/del/diccionario.list"
scanner = Scanner(url)
